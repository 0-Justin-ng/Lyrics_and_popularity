{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Appends the entire brainstation_capstone project folder to the path.\n",
    "# This allows us to make a relative import of our scripts in brainstation_capstone/scripts\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utilities import utils\n",
    "from utilities.vectorizer_pipeline import VectorizerPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = utils.get_datapath('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [**2. Transforming Lyrics**](#toc1_)    \n",
    "- [Vectorizing Lyrics for Classification](#toc2_)    \n",
    "- [Combining all Vectorizers](#toc3_)    \n",
    "- [Converting Into LexVec Word Embeddings](#toc4_)    \n",
    "- [Conclusion](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[**2. Transforming Lyrics**](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the preliminary work of vectorizing the data. A class was created to vectorize a train set and transform the validation and test set. This class would then store transformed train, validation and test set for preliminary modeling. The ideology for this method was to stay true to the Agile methodology, where we will try to try as many vectorizer combinations and narrow down one to tune a model for after.\n",
    "\n",
    "Specifically, we will look at the following transformations:\n",
    "- CountVectorizer (N-grams = 1, 2 and 3)\n",
    "- TF-IDF\n",
    "- Averaging LexVec Embeddings\n",
    "- OpenAI Ada Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH / 'clean_lyrics_english_stem.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>release_year</th>\n",
       "      <th>title</th>\n",
       "      <th>primary_artist</th>\n",
       "      <th>views</th>\n",
       "      <th>cleaned_lyrics</th>\n",
       "      <th>language</th>\n",
       "      <th>log_scaled_views</th>\n",
       "      <th>popular</th>\n",
       "      <th>popularity_three_class</th>\n",
       "      <th>cleaned_lyrics_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kendrick-lamar-swimming-pools-drank-lyrics</td>\n",
       "      <td>\\n\\n[Produced by T-Minus]\\n\\n[Intro]\\nPour up ...</td>\n",
       "      <td>2012</td>\n",
       "      <td>Swimming Pools (Drank)</td>\n",
       "      <td>Kendrick-lamar</td>\n",
       "      <td>5589280.0</td>\n",
       "      <td>pour up drank head shot drank sit down drank ...</td>\n",
       "      <td>en</td>\n",
       "      <td>15.536361</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>pour drank head shot drank sit drank stand dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kendrick-lamar-money-trees-lyrics</td>\n",
       "      <td>\\n\\n[Produced by DJ Dahi]\\n\\n[Verse 1: Kendric...</td>\n",
       "      <td>2012</td>\n",
       "      <td>Money Trees</td>\n",
       "      <td>Kendrick-lamar</td>\n",
       "      <td>4592003.0</td>\n",
       "      <td>uh me and my niggas tryna get it ya bish ya b...</td>\n",
       "      <td>en</td>\n",
       "      <td>15.339827</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>uh nigga tryna get ya bish ya bish hit hous l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kendrick-lamar-xxx-lyrics</td>\n",
       "      <td>\\n\\n[Intro: Bēkon &amp; Kid Capri]\\nAmerica, God b...</td>\n",
       "      <td>2017</td>\n",
       "      <td>XXX.</td>\n",
       "      <td>Kendrick-lamar</td>\n",
       "      <td>4651514.0</td>\n",
       "      <td>america god bless you if its good to you amer...</td>\n",
       "      <td>en</td>\n",
       "      <td>15.352703</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>america god bless good america pleas take han...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A-ap-rocky-fuckin-problems-lyrics</td>\n",
       "      <td>\\n\\n[Chorus: 2 Chainz, Drake &amp; Both (A$AP Rock...</td>\n",
       "      <td>2012</td>\n",
       "      <td>Fuckin’ Problems</td>\n",
       "      <td>A-ap-rocky</td>\n",
       "      <td>7378309.0</td>\n",
       "      <td>i love bad bitches thats my fuckin problem an...</td>\n",
       "      <td>en</td>\n",
       "      <td>15.814055</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>love bad bitch that fuckin problem yeah like ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kendrick-lamar-dna-lyrics</td>\n",
       "      <td>\\n\\n[Verse 1]\\nI got, I got, I got, I got—\\nLo...</td>\n",
       "      <td>2017</td>\n",
       "      <td>DNA.</td>\n",
       "      <td>Kendrick-lamar</td>\n",
       "      <td>5113687.0</td>\n",
       "      <td>i got i got i got i got loyalty got royalty i...</td>\n",
       "      <td>en</td>\n",
       "      <td>15.447431</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>got got got got loyalti got royalti insid dna...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         song  \\\n",
       "0  Kendrick-lamar-swimming-pools-drank-lyrics   \n",
       "1           Kendrick-lamar-money-trees-lyrics   \n",
       "2                   Kendrick-lamar-xxx-lyrics   \n",
       "3           A-ap-rocky-fuckin-problems-lyrics   \n",
       "4                   Kendrick-lamar-dna-lyrics   \n",
       "\n",
       "                                              lyrics  release_year  \\\n",
       "0  \\n\\n[Produced by T-Minus]\\n\\n[Intro]\\nPour up ...          2012   \n",
       "1  \\n\\n[Produced by DJ Dahi]\\n\\n[Verse 1: Kendric...          2012   \n",
       "2  \\n\\n[Intro: Bēkon & Kid Capri]\\nAmerica, God b...          2017   \n",
       "3  \\n\\n[Chorus: 2 Chainz, Drake & Both (A$AP Rock...          2012   \n",
       "4  \\n\\n[Verse 1]\\nI got, I got, I got, I got—\\nLo...          2017   \n",
       "\n",
       "                    title  primary_artist      views  \\\n",
       "0  Swimming Pools (Drank)  Kendrick-lamar  5589280.0   \n",
       "1             Money Trees  Kendrick-lamar  4592003.0   \n",
       "2                    XXX.  Kendrick-lamar  4651514.0   \n",
       "3        Fuckin’ Problems      A-ap-rocky  7378309.0   \n",
       "4                    DNA.  Kendrick-lamar  5113687.0   \n",
       "\n",
       "                                      cleaned_lyrics language  \\\n",
       "0   pour up drank head shot drank sit down drank ...       en   \n",
       "1   uh me and my niggas tryna get it ya bish ya b...       en   \n",
       "2   america god bless you if its good to you amer...       en   \n",
       "3   i love bad bitches thats my fuckin problem an...       en   \n",
       "4   i got i got i got i got loyalty got royalty i...       en   \n",
       "\n",
       "   log_scaled_views  popular  popularity_three_class  \\\n",
       "0         15.536361        1                       2   \n",
       "1         15.339827        1                       2   \n",
       "2         15.352703        1                       2   \n",
       "3         15.814055        1                       2   \n",
       "4         15.447431        1                       2   \n",
       "\n",
       "                                 cleaned_lyrics_stem  \n",
       "0   pour drank head shot drank sit drank stand dr...  \n",
       "1   uh nigga tryna get ya bish ya bish hit hous l...  \n",
       "2   america god bless good america pleas take han...  \n",
       "3   love bad bitch that fuckin problem yeah like ...  \n",
       "4   got got got got loyalti got royalti insid dna...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(33842, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Vectorizing Lyrics for Classification](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we prepare a vectorizer pipeline for a CountVectorizer with varying lengths of n_grams, along with TF-IDF. We do this for both the binary and multi-class problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33842,), (33842,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_popular = df.popular\n",
    "y_popularity = df.popularity_three_class\n",
    "\n",
    "y_popular.shape, y_popularity.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set the correct targets for the two class and three class for Genius page views. These were used in the supplementary preliminary modelling. We now make vectorizer objects that can store the vectorizer, the X_train, X_validation, X_test and all the corresponding y values for each split. More info can be found in `utilities.vectorizer_pipeline.py`. We vectorize for both the two class and three class for the preliminary modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (20304, 2212)             \n",
      "Validation shape: (6769, 2212)             \n",
      "Test shape: (6769, 2212)\n",
      "Transformed train test split dumped at /home/jng/projects/brainstation_capstone/vectorizer_data/bag_of_words_two_class/data.pkl as a dictionary.\n",
      "Train shape: (20304, 2204)             \n",
      "Validation shape: (6769, 2204)             \n",
      "Test shape: (6769, 2204)\n",
      "Transformed train test split dumped at /home/jng/projects/brainstation_capstone/vectorizer_data/tf_idf_two_class/data.pkl as a dictionary.\n",
      "Train shape: (20304, 2968)             \n",
      "Validation shape: (6769, 2968)             \n",
      "Test shape: (6769, 2968)\n",
      "Transformed train test split dumped at /home/jng/projects/brainstation_capstone/vectorizer_data/2_grams_two_class/data.pkl as a dictionary.\n",
      "Train shape: (20304, 3008)             \n",
      "Validation shape: (6769, 3008)             \n",
      "Test shape: (6769, 3008)\n",
      "Transformed train test split dumped at /home/jng/projects/brainstation_capstone/vectorizer_data/3_grams_two_class/data.pkl as a dictionary.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for vectorizer_name in [\n",
    "    'bag_of_words_two_class', 'tf_idf_two_class', '2_grams_two_class', '3_grams_two_class'\n",
    "    ]:\n",
    "    X = df.cleaned_lyrics_stem\n",
    "    \n",
    "    if vectorizer_name == 'bag_of_words_two_class':\n",
    "        vectorizer = CountVectorizer(max_df=0.9, min_df=0.01)\n",
    "    elif vectorizer_name == 'tf_idf_two_class':\n",
    "        vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.01)\n",
    "    elif vectorizer_name == '2_grams_two_class':\n",
    "        vectorizer = CountVectorizer(max_df=0.9, min_df=0.01, ngram_range=(1,2))\n",
    "    elif vectorizer_name == '3_grams_two_class':\n",
    "        vectorizer = CountVectorizer(max_df=0.9, min_df=0.01, ngram_range=(1,3))\n",
    "    \n",
    "    VectorizerPipeline(\n",
    "        vectorizer_name, vectorizer, X, y_popular\n",
    "    ).run_vectorizer_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jng/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (22743, 2152)             \n",
      "Validation shape: (7581, 2152)             \n",
      "Test shape: (7581, 2152)\n",
      "Transformed train test split dumped at /home/jng/projects/brainstation_capstone/vectorizer_data/bag_of_words_three_class/data.pkl as a dictionary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jng/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (22743, 2143)             \n",
      "Validation shape: (7581, 2143)             \n",
      "Test shape: (7581, 2143)\n",
      "Transformed train test split dumped at /home/jng/projects/brainstation_capstone/vectorizer_data/tf_idf_three_class/data.pkl as a dictionary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jng/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (22743, 2591)             \n",
      "Validation shape: (7581, 2591)             \n",
      "Test shape: (7581, 2591)\n",
      "Transformed train test split dumped at /home/jng/projects/brainstation_capstone/vectorizer_data/2_grams_three_class/data.pkl as a dictionary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jng/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (22743, 2587)             \n",
      "Validation shape: (7581, 2587)             \n",
      "Test shape: (7581, 2587)\n",
      "Transformed train test split dumped at /home/jng/projects/brainstation_capstone/vectorizer_data/3_grams_three_class/data.pkl as a dictionary.\n"
     ]
    }
   ],
   "source": [
    "for vectorizer_name in [\n",
    "    'bag_of_words_three_class', 'tf_idf_three_class', '2_grams_three_class', '3_grams_three_class'\n",
    "    ]:\n",
    "    X = df.cleaned_lyrics_stem\n",
    "    \n",
    "    if vectorizer_name == 'bag_of_words_three_class':\n",
    "        vectorizer = CountVectorizer(max_df=0.9, min_df=0.01)\n",
    "    elif vectorizer_name == 'tf_idf_three_class':\n",
    "        vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.01)\n",
    "    elif vectorizer_name == '2_grams_three_class':\n",
    "        vectorizer = CountVectorizer(max_df=0.9, min_df=0.01, ngram_range=(1,2))\n",
    "    elif vectorizer_name == '3_grams_three_class':\n",
    "        vectorizer = CountVectorizer(max_df=0.9, min_df=0.01, ngram_range=(1,3))\n",
    "    \n",
    "    VectorizerPipeline(\n",
    "        vectorizer_name, vectorizer, X, y_popularity\n",
    "    ).run_vectorizer_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Combining all Vectorizers](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also try to combine all the vectorizers as another representation of the lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (20304, 5207)             \n",
      "Validation shape: (6769, 5207)             \n",
      "Test shape: (6769, 5207)\n",
      "Transformed train test split dumped at /home/jng/projects/brainstation_capstone/vectorizer_data/all_vectorizers_three_class/data.pkl as a dictionary.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "X = df.cleaned_lyrics_stem\n",
    "\n",
    "# Instantiate a list of tuples - each tuple is the name of the transform + the transformer\n",
    "vectorizers = [\n",
    "    ('count_vect', CountVectorizer(max_df=0.9, min_df=0.01, ngram_range=(1,3))), \n",
    "    ('tfidf', TfidfVectorizer(max_df=0.9, min_df=0.01 ))\n",
    "    ]\n",
    "\n",
    "# Create feature union\n",
    "featunion = FeatureUnion(vectorizers)\n",
    "\n",
    "VectorizerPipeline(\n",
    "    'all_vectorizers_three_class', featunion, X, y_popularity\n",
    ").run_vectorizer_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Converting Into LexVec Word Embeddings](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we provide the process of averaging LexVec Word Embeddings for each word in the lyrics to form a single document embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "stop_words = text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gensim\n",
    "\n",
    "# Instantiate the LexVec Embeddings.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    DATA_PATH / 'lexvec-wikipedia-word-vectors', binary=False\n",
    ")\n",
    "\n",
    "def lyric2vec(lyric):\n",
    "    \"\"\"\n",
    "    Embed a lyric by averaging the word vectors of the lyrics for each song. \n",
    "    Out-of-vocabulary words are replaced by a zero-vector.\n",
    "    -----\n",
    "    \n",
    "    Input: lyric (string)\n",
    "    Output: document embedding vector (np.array)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    word_embeddings = [np.zeros(300)]\n",
    "    for word in lyric:\n",
    "        # If word is in stop words ignore it.\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        # if the word is in the model then embed\n",
    "        elif word in model:\n",
    "            vector = model[word]\n",
    "        # add zeros for out-of-vocab words\n",
    "        else:\n",
    "            vector = np.zeros(300)\n",
    "            \n",
    "        word_embeddings.append(vector)\n",
    "    \n",
    "    # average the word vectors\n",
    "    sentence_embedding = np.stack(word_embeddings).mean(axis=0)\n",
    "    sentence_embedding.reshape(1,300)\n",
    "    \n",
    "    return sentence_embedding\n",
    "\n",
    "\n",
    "# Average the word embeddings in each lyric to get the document embedding. \n",
    "word_embedding_lyrics = [\n",
    "    lyric2vec(lyric)\n",
    "    for lyric in df['cleaned_lyrics']\n",
    "]\n",
    "\n",
    "final_lexvec = np.array(word_embedding_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37905, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_lexvec.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have all the embeddings we dump the embeddings using `joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "LEXVEC_PATH = utils.get_datapath('lexVec_data')\n",
    "\n",
    "with open(\n",
    "    LEXVEC_PATH / 'lexVec.pkl',\n",
    "    'wb'\n",
    ") as f:\n",
    "    joblib.dump(final_lexvec, f)\n",
    "    \n",
    "    print(f\"LexVec data dumped at {LEXVEC_PATH / 'lexVec.pkl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Conclusion](#toc0_)\n",
    "\n",
    "Now that we have all our representations we can go into some preliminary modeling. The preliminary modelling will be a quick first pass to see how various models perform using the vectorized data above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7b969017e21ea3fceb440bc3568f42d726fda0cedf260d07c9604e5a4a75144"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
