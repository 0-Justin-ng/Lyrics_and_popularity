{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Appends the entire brainstation_capstone project folder to the path.\n",
    "# This allows us to make a relative import of our scripts in brainstation_capstone/scripts\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utilities import utils\n",
    "from utilities.vectorizer_pipeline import VectorizerPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = utils.get_datapath('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [**2. Transforming Lyrics**](#toc1_)    \n",
    "- [Vectorizing Lyrics for Classification](#toc2_)    \n",
    "- [Combining all Vectorizers](#toc3_)    \n",
    "- [Converting Into LexVec Word Embeddings](#toc4_)    \n",
    "- [Open AI Embeddings](#toc5_)    \n",
    "- [Conclusion](#toc6_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[**2. Transforming Lyrics**](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the preliminary work of vectorizing the data. A class was created to vectorize a train set and transform the validation and test set. This class would then store transformed train, validation and test set for preliminary modeling. The ideology for this method was to stay true to the Agile methodology, where we will try to try as many vectorizer combinations and narrow down one to tune a model for after.\n",
    "\n",
    "Specifically, we will look at the following transformations:\n",
    "- CountVectorizer (N-grams = 1, 2 and 3)\n",
    "- TF-IDF\n",
    "- Averaging LexVec Embeddings\n",
    "- OpenAI Ada Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH / 'clean_lyrics_english_stem.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>song</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>views</th>\n",
       "      <th>cleaned_lyrics</th>\n",
       "      <th>log_scaled_views</th>\n",
       "      <th>popular</th>\n",
       "      <th>popularity_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Kendrick-lamar-swimming-pools-drank-lyrics</td>\n",
       "      <td>\\n\\n[Produced by T-Minus]\\n\\n[Intro]\\nPour up ...</td>\n",
       "      <td>5589280.0</td>\n",
       "      <td>pour up drank head shot drank sit down drank ...</td>\n",
       "      <td>15.536361</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Kendrick-lamar-money-trees-lyrics</td>\n",
       "      <td>\\n\\n[Produced by DJ Dahi]\\n\\n[Verse 1: Kendric...</td>\n",
       "      <td>4592003.0</td>\n",
       "      <td>uh me and my niggas tryna get it ya bish ya b...</td>\n",
       "      <td>15.339827</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Kendrick-lamar-xxx-lyrics</td>\n",
       "      <td>\\n\\n[Intro: Bēkon &amp; Kid Capri]\\nAmerica, God b...</td>\n",
       "      <td>4651514.0</td>\n",
       "      <td>america god bless you if its good to you amer...</td>\n",
       "      <td>15.352703</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A-ap-rocky-fuckin-problems-lyrics</td>\n",
       "      <td>\\n\\n[Chorus: 2 Chainz, Drake &amp; Both (A$AP Rock...</td>\n",
       "      <td>7378309.0</td>\n",
       "      <td>i love bad bitches thats my fuckin problem an...</td>\n",
       "      <td>15.814055</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Kendrick-lamar-dna-lyrics</td>\n",
       "      <td>\\n\\n[Verse 1]\\nI got, I got, I got, I got—\\nLo...</td>\n",
       "      <td>5113687.0</td>\n",
       "      <td>i got i got i got i got loyalty got royalty i...</td>\n",
       "      <td>15.447431</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        song  \\\n",
       "0           0  Kendrick-lamar-swimming-pools-drank-lyrics   \n",
       "1           1           Kendrick-lamar-money-trees-lyrics   \n",
       "2           2                   Kendrick-lamar-xxx-lyrics   \n",
       "3           3           A-ap-rocky-fuckin-problems-lyrics   \n",
       "4           4                   Kendrick-lamar-dna-lyrics   \n",
       "\n",
       "                                              lyrics      views  \\\n",
       "0  \\n\\n[Produced by T-Minus]\\n\\n[Intro]\\nPour up ...  5589280.0   \n",
       "1  \\n\\n[Produced by DJ Dahi]\\n\\n[Verse 1: Kendric...  4592003.0   \n",
       "2  \\n\\n[Intro: Bēkon & Kid Capri]\\nAmerica, God b...  4651514.0   \n",
       "3  \\n\\n[Chorus: 2 Chainz, Drake & Both (A$AP Rock...  7378309.0   \n",
       "4  \\n\\n[Verse 1]\\nI got, I got, I got, I got—\\nLo...  5113687.0   \n",
       "\n",
       "                                      cleaned_lyrics  log_scaled_views  \\\n",
       "0   pour up drank head shot drank sit down drank ...         15.536361   \n",
       "1   uh me and my niggas tryna get it ya bish ya b...         15.339827   \n",
       "2   america god bless you if its good to you amer...         15.352703   \n",
       "3   i love bad bitches thats my fuckin problem an...         15.814055   \n",
       "4   i got i got i got i got loyalty got royalty i...         15.447431   \n",
       "\n",
       "   popular  popularity_rating  \n",
       "0        1                  2  \n",
       "1        1                  2  \n",
       "2        1                  2  \n",
       "3        1                  2  \n",
       "4        1                  2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(37905, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Vectorizing Lyrics for Classification](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we prepare a vectorizer pipeline for a CountVectorizer with varying lengths of n_grams, along with TF-IDF. We do this for both the binary and multi-class problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37905,), (37905,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_popular = df.popular\n",
    "y_popularity = df.popularity_rating\n",
    "\n",
    "y_popular.shape, y_popularity.shape\n",
    "\n",
    "for vectorizer_name in [\n",
    "    'bag_of_words_two_class', 'tf_idf_two_class', '2_grams_two_class', '3_grams_two_class'\n",
    "    ]:\n",
    "    X = df.cleaned_lyrics\n",
    "    \n",
    "    if vectorizer_name == 'bag_of_words_two_class':\n",
    "        vectorizer = CountVectorizer(max_df=0.9, min_df=0.01)\n",
    "    elif vectorizer_name == 'tf_idf_two_class':\n",
    "        vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.01)\n",
    "    elif vectorizer_name == '2_grams_two_class':\n",
    "        vectorizer = CountVectorizer(max_df=0.9, min_df=0.01, ngram_range=(1,2))\n",
    "    elif vectorizer_name == '3_grams_two_class':\n",
    "        vectorizer = CountVectorizer(max_df=0.9, min_df=0.01, ngram_range=(1,3))\n",
    "    \n",
    "    VectorizerPipeline(\n",
    "        vectorizer_name, vectorizer, X, y_popular\n",
    "    ).run_vectorizer_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jng/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (22743, 2152)             \n",
      "Validation shape: (7581, 2152)             \n",
      "Test shape: (7581, 2152)\n",
      "Transformed train test split dumped at /home/jng/projects/brainstation_capstone/vectorizer_data/bag_of_words_three_class/data.pkl as a dictionary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jng/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (22743, 2143)             \n",
      "Validation shape: (7581, 2143)             \n",
      "Test shape: (7581, 2143)\n",
      "Transformed train test split dumped at /home/jng/projects/brainstation_capstone/vectorizer_data/tf_idf_three_class/data.pkl as a dictionary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jng/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (22743, 2591)             \n",
      "Validation shape: (7581, 2591)             \n",
      "Test shape: (7581, 2591)\n",
      "Transformed train test split dumped at /home/jng/projects/brainstation_capstone/vectorizer_data/2_grams_three_class/data.pkl as a dictionary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jng/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (22743, 2587)             \n",
      "Validation shape: (7581, 2587)             \n",
      "Test shape: (7581, 2587)\n",
      "Transformed train test split dumped at /home/jng/projects/brainstation_capstone/vectorizer_data/3_grams_three_class/data.pkl as a dictionary.\n"
     ]
    }
   ],
   "source": [
    "for vectorizer_name in [\n",
    "    'bag_of_words_three_class', 'tf_idf_three_class', '2_grams_three_class', '3_grams_three_class'\n",
    "    ]:\n",
    "    X = df.cleaned_lyrics\n",
    "    \n",
    "    if vectorizer_name == 'bag_of_words_three_class':\n",
    "        vectorizer = CountVectorizer(max_df=0.9, min_df=0.01)\n",
    "    elif vectorizer_name == 'tf_idf_three_class':\n",
    "        vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.01)\n",
    "    elif vectorizer_name == '2_grams_three_class':\n",
    "        vectorizer = CountVectorizer(max_df=0.9, min_df=0.01, ngram_range=(1,2))\n",
    "    elif vectorizer_name == '3_grams_three_class':\n",
    "        vectorizer = CountVectorizer(max_df=0.9, min_df=0.01, ngram_range=(1,3))\n",
    "    \n",
    "    VectorizerPipeline(\n",
    "        vectorizer_name, vectorizer, X, y_popularity\n",
    "    ).run_vectorizer_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Combining all Vectorizers](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also try to combine all the vectorizers as another representation of the lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jng/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (22743, 4778)             \n",
      "Validation shape: (7581, 4778)             \n",
      "Test shape: (7581, 4778)\n",
      "Transformed train test split dumped at /home/jng/projects/brainstation_capstone/vectorizer_data/all_vectorizers_three_class/data.pkl as a dictionary.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "X = df.cleaned_lyrics\n",
    "\n",
    "# Instantiate a list of tuples - each tuple is the name of the transform + the transformer\n",
    "vectorizers = [\n",
    "    ('count_vect', CountVectorizer(max_df=0.9, min_df=0.01, ngram_range=(1,3))), \n",
    "    ('tfidf', TfidfVectorizer(max_df=0.9, min_df=0.01 ))\n",
    "    ]\n",
    "\n",
    "# Create feature union\n",
    "featunion = FeatureUnion(vectorizers)\n",
    "\n",
    "VectorizerPipeline(\n",
    "    'all_vectorizers_three_class', featunion, X, y_popularity\n",
    ").run_vectorizer_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Converting Into LexVec Word Embeddings](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we provide the process of averaging LexVec Word Embeddings for each word in the lyrics to form a single document embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gensim\n",
    "\n",
    "# Instantiate the LexVec Embeddings.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    DATA_PATH / 'lexvec-wikipedia-word-vectors', binary=False\n",
    ")\n",
    "\n",
    "def lyric2vec(lyric):\n",
    "    \"\"\"\n",
    "    Embed a lyric by averaging the word vectors of the lyrics for each song. \n",
    "    Out-of-vocabulary words are replaced by a zero-vector.\n",
    "    -----\n",
    "    \n",
    "    Input: lyric (string)\n",
    "    Output: document embedding vector (np.array)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    word_embeddings = [np.zeros(300)]\n",
    "    for word in lyric:\n",
    "        # If word is in stop words ignore it.\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        # if the word is in the model then embed\n",
    "        elif word in model:\n",
    "            vector = model[word]\n",
    "        # add zeros for out-of-vocab words\n",
    "        else:\n",
    "            vector = np.zeros(300)\n",
    "            \n",
    "        word_embeddings.append(vector)\n",
    "    \n",
    "    # average the word vectors\n",
    "    sentence_embedding = np.stack(word_embeddings).mean(axis=0)\n",
    "    sentence_embedding.reshape(1,300)\n",
    "    \n",
    "    return sentence_embedding\n",
    "\n",
    "\n",
    "# Average the word embeddings in each lyric to get the document embedding. \n",
    "word_embedding_lyrics = [\n",
    "    lyric2vec(lyric)\n",
    "    for lyric in df['cleaned_lyrics']\n",
    "]\n",
    "\n",
    "final_lexvec = np.array(word_embedding_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37905, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_lexvec.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have all the embeddings we dump the embeddings using `joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "LEXVEC_PATH = utils.get_datapath('lexVec_data')\n",
    "\n",
    "with open(\n",
    "    LEXVEC_PATH / 'lexVec.pkl',\n",
    "    'wb'\n",
    ") as f:\n",
    "    joblib.dump(final_lexvec, f)\n",
    "    \n",
    "    print(f\"LexVec data dumped at {LEXVEC_PATH / 'lexVec.pkl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Open AI Embeddings](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final transformation of the lyrics to try is using an Open AI embedding model. \n",
    "\n",
    "To do this we need to first initialize our connection to the Open AI api. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import openai\n",
    "\n",
    "# Reading the OpenAI api key. \n",
    "with open(DATA_PATH / 'open_ai_key.txt', 'r') as file:\n",
    "    openai_api_key = file.readline()\n",
    "\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Setting the model parameters for the embeddings.\n",
    "embedding_model = 'text-embedding-ada-002'\n",
    "embedding_encoding = 'cl100k_base' # Tokenizer for the above ada embedding model. \n",
    "max_tokens = 8000 # This is the max token limit for the Ada Embedding.\n",
    "\n",
    "df = pd.read_csv(DATA_PATH / 'clean_lyrics_and_spotify.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35908, 14)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After intializing our connection to the Open AI api, we need to make sure all our lyrics stay below the 8000 token limit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(embedding_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n_tokens'] = df['cleaned_lyrics'].apply(lambda x: len(encoding.encode(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to limit lyrics to those below the max token limit for the second generation ada embedding model.\n",
    "df = df[\n",
    "    df['n_tokens'] <= max_tokens\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35901, 15)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have filtered out all the songs that are above the token limit we can proceed to access the api and get the document embeddings for our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "# These packages are used to prevent us from reaching the rate limit when accessing the API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates an Ada embedding given text as input. \n",
    "# The decorator allows us to pause the number of requests that we are making to the api\n",
    "# if we reach the rate limit. \n",
    "@retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=4, max=30))\n",
    "def get_embedding(text, engine=\"text-embedding-ada-002\"):\n",
    "    response = openai.Embedding.create(input=[text], model=engine)\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35901/35901 [2:07:10<00:00,  4.70it/s]  \n"
     ]
    }
   ],
   "source": [
    "df['ada_embeddings'] = df['cleaned_lyrics'].progress_apply(lambda x: get_embedding(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(DATA_PATH / 'clean_lyrics_spotify_ada.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Conclusion](#toc0_)\n",
    "\n",
    "Now that we have all our representations we can go into some preliminary modeling. Note the Ada embeddings were not included in the preliminary modeling in `notebooks/3_prelim_modelling.ipynb`, as they were added after the preliminary modeling was already completed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7b969017e21ea3fceb440bc3568f42d726fda0cedf260d07c9604e5a4a75144"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
