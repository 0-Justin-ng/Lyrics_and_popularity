{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Appends the entire brainstation_capstone project folder to the path.\n",
    "# This allows us to make a relative import of our scripts in brainstation_capstone/scripts\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utilities import utils\n",
    "from utilities.vectorizer_pipeline import VectorizerPipeline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "DATA_PATH = utils.get_datapath('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [**4. Generating OpenAI Embeddings**](#toc1_)    \n",
    "- [Loading the Dataset](#toc2_)    \n",
    "- [Initialize Connection to OpenAI API](#toc3_)    \n",
    "- [Checking for Lyrics Over the Max Token Limit](#toc4_)    \n",
    "- [Generating Document Embeddings](#toc5_)    \n",
    "- [Conclusion](#toc6_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[**4. Generating OpenAI Embeddings**](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final transformation of the lyrics to try is using an OpenAI embedding model. Specifically we are using the second generation Ada embedding which boasts high performance for text classification tasks. Find more info [here](https://openai.com/blog/new-and-improved-embedding-model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Loading the Dataset](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will embed the lyrics that are not stemmed and do not have stop words removed. This decision was made as the embeddings themselves can capture the differences between words that stemming would reduce to the same word. For example, the embedding will provide different results for `run` and `running` but stemming would reduce both these words to `run`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33842, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH / 'clean_lyrics_english.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Initialize Connection to OpenAI API](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first step to generate the document embeddings is to initialize our connection to the OpenAI API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import openai\n",
    "\n",
    "# Reading the OpenAI API key. \n",
    "with open(DATA_PATH / 'open_ai_key.txt', 'r') as file:\n",
    "    openai_API_key = file.readline()\n",
    "\n",
    "openai.API_key = openai_API_key\n",
    "\n",
    "# Setting the model parameters for the embeddings.\n",
    "embedding_model = 'text-embedding-ada-002'\n",
    "embedding_encoding = 'cl100k_base' # Tokenizer for the above ada embedding model. \n",
    "max_tokens = 8000 # This is the max token limit for the Ada Embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Checking for Lyrics Over the Max Token Limit](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After intializing our connection to the OpenAI API, we need to make sure all our lyrics stay below the 8000 token limit set for the embedding model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(embedding_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n_tokens'] = df['cleaned_lyrics'].apply(lambda x: len(encoding.encode(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to limit lyrics to those below the max token limit for the second generation ada embedding model.\n",
    "df = df[\n",
    "    df['n_tokens'] <= max_tokens\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35901, 15)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Generating Document Embeddings](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have filtered out all the songs that are above the token limit we can proceed to access the API and get the document embeddings for our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These packages are used to prevent us from reaching the rate limit when accessing the API. \n",
    "from tenacity import retry, stop_after_attempt, wait_exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates an Ada embedding given text as input. \n",
    "# The decorator allows us to pause the number of requests that we are making to the API\n",
    "# if we reach the rate limit. \n",
    "@retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=4, max=30))\n",
    "def get_embedding(text, engine=\"text-embedding-ada-002\"):\n",
    "    response = openai.Embedding.create(input=[text], model=engine)\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35901/35901 [2:07:10<00:00,  4.70it/s]  \n"
     ]
    }
   ],
   "source": [
    "df['ada_embeddings'] = df['cleaned_lyrics'].progress_apply(lambda x: get_embedding(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(DATA_PATH / 'clean_lyrics_ada.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Conclusion](#toc0_)\n",
    "\n",
    "From the above we have successfully generated document embeddings for all our lyrics using a second generation Ada text embedding from OpenAI. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
